{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb83d931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from huggingface_hub import login, notebook_login\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "import pypdf\n",
    "import gradio as gr\n",
    "from IPython.display import display, Markdown\n",
    "import os\n",
    "from pathlib import Path\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13702a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66068e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a732791a1b04cba80b248382957044f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9433fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_markdown(text):\n",
    "    \"\"\"Display text as Markdown\"\"\"\n",
    "    display(Markdown(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0dadaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'positive', 'score': 0.9498746395111084}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = pipeline(model=\"ProsusAI/finbert\")\n",
    "\n",
    "pipe(\"My Bitcoin is up quite a bit from when I bought it!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd7250dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'neutral', 'score': 0.9375986456871033}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"NVDIA is set to release their earnings soon.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6883e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"microsoft/Phi-4-mini-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ef4abed",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f9fca51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ff65990a23b4b37b135c9e7309c0db8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    device_map = \"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0aeffbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(\"Hello.  I'm working on LLMs, fine-tuning and AI agents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "305069fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 13, 220, 314, 1101, 1762, 319, 27140, 10128, 11, 3734, 12, 28286, 278, 290, 9552, 6554]\n"
     ]
    }
   ],
   "source": [
    "print(tokens['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12fdf968",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_url = \"https://abc.xyz/assets/66/ae/c94682fc4137b5fb90a5d709ac4b/2025-q1-earnings-transcript.pdf\"\n",
    "\n",
    "pdf_filename = \"google_earning_transcript\"\n",
    "\n",
    "pdf_path = Path(pdf_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a5987ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF file already exists at google_earning_transcript\n"
     ]
    }
   ],
   "source": [
    "if not pdf_path.exists():\n",
    "    response = requests.get(pdf_url)\n",
    "    response.raise_for_status()\n",
    "    pdf_path.write_bytes(response.content)\n",
    "    print(f\"PDF downloaded successfully to {pdf_path}\")\n",
    "else:\n",
    "    print(f\"PDF file already exists at {pdf_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8e8819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_text = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e2abf2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 572 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF has 21 pages.\n"
     ]
    }
   ],
   "source": [
    "reader = pypdf.PdfReader(pdf_path)\n",
    "num_pages = len(reader.pages)\n",
    "\n",
    "print(f\"PDF has {num_pages} pages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c5bbbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pages_text = []\n",
    "\n",
    "for i, page in enumerate(reader.pages):\n",
    "    page_text = page.extract_text()\n",
    "    if page_text:\n",
    "        all_pages_text.append(page_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "526585df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 64652\n"
     ]
    }
   ],
   "source": [
    "pdf_text = \"\\n\".join(all_pages_text)\n",
    "print(f\"Total characters: {len(pdf_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd6a4dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "rn  the  conference  back  over  to  Jim  Friedland  for  any  further  remarks.    Jim  Friedland,  Senior  Director,  Investor  Relations:  Thanks,  everyone,  for  joining  us  today.  \n",
       "We\n",
       " \n",
       "look\n",
       " \n",
       "forward\n",
       " \n",
       "to\n",
       " \n",
       "speaking\n",
       " \n",
       "with\n",
       " \n",
       "you\n",
       " \n",
       "again\n",
       " \n",
       "on\n",
       " \n",
       "our\n",
       " \n",
       "second\n",
       " \n",
       "quarter\n",
       " \n",
       "2025\n",
       " \n",
       "call.\n",
       " \n",
       "Thank\n",
       " \n",
       "you,\n",
       " \n",
       "and\n",
       " \n",
       "have\n",
       " \n",
       "a\n",
       " \n",
       "good\n",
       " \n",
       "evening.\n",
       " \n",
       "  Operator:  Thank  you,  everyone.  This  concludes  today's  conference  call.  Thank  you  for  \n",
       "participating.\n",
       " \n",
       "You\n",
       " \n",
       "may\n",
       " \n",
       "now\n",
       " \n",
       "disconnect.\n",
       " \n",
       "   \n",
       "21  "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_markdown(f\"{pdf_text[-500:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75dc3998",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MAX_CONTEXT_CHARS = 6000\n",
    "\n",
    "def answer_question_from_pdf(document_text, question, llm_pipeline):\n",
    "    \"\"\"\n",
    "    Answers a question based on the provided document text using the loaded LLM pipeline.\n",
    "\n",
    "    Args:\n",
    "        document_text (str): The text extracted from the PDF.\n",
    "        question (str): The user's question.\n",
    "        llm_pipeline (transformers.pipeline): The initialized text-generation pipeline.\n",
    "\n",
    "    Returns:\n",
    "        str: The model's generated answer.\n",
    "    \"\"\"\n",
    "    # Truncate context if necessary\n",
    "    if len(document_text) > MAX_CONTEXT_CHARS:\n",
    "        print(f\"Warning: Document text ({len(document_text)} chars) exceeds limit ({MAX_CONTEXT_CHARS} chars). Truncating.\")\n",
    "        context = document_text[:MAX_CONTEXT_CHARS] + \"...\"\n",
    "    else:\n",
    "        context = document_text\n",
    "\n",
    "    # Let's define the Prompt Template\n",
    "    # We instruct the model to use only the provided document.\n",
    "    # Using a format the model expects (like Phi-3's chat format) can improve results.\n",
    "    # <|system|> provides context/instructions, <|user|> is the question.\n",
    "    # Note: Different models might prefer different prompt structures.\n",
    "    prompt_template = f\"\"\"<|system|>\n",
    "    You are an AI assistant. Answer the following question based *only* on the provided document text. If the answer is not found in the document, say \"The document does not contain information on this topic.\" Do not use any prior knowledge.\n",
    "\n",
    "    Document Text:\n",
    "    ---\n",
    "    {context}\n",
    "    ---\n",
    "    <|end|>\n",
    "    <|user|>\n",
    "    Question: {question}<|end|>\n",
    "    <|assistant|>\n",
    "    Answer:\"\"\" # We prompt the model to start generating the answer\n",
    "\n",
    "    print(f\"\\n--- Generating Answer for: '{question}' ---\")\n",
    "\n",
    "    # Run Inference on the chosen model\n",
    "    outputs = llm_pipeline(prompt_template,\n",
    "                        #    max_new_tokens = 500,  # Limit answer length\n",
    "                        #    do_sample = True,\n",
    "                        #    temperature = 0.2,   # Lower temperature for more factual Q&A\n",
    "                        #    top_p = 0.9\n",
    "                        )\n",
    "\n",
    "    # Let's extract the answer\n",
    "    # The output includes the full prompt template. We need the text generated *after* it.\n",
    "    full_generated_text = outputs[0]['generated_text']\n",
    "    answer_start_index = full_generated_text.find(\"Answer:\") + len(\"Answer:\")\n",
    "    raw_answer = full_generated_text[answer_start_index:].strip()\n",
    "\n",
    "    # Sometimes the model might still include parts of the prompt or trail off.\n",
    "    # Basic cleanup: Find the end-of-sequence token if possible, or just return raw.\n",
    "    # Phi-3 uses <|end|> or <|im_end|>\n",
    "    end_token = \"<|end|>\"\n",
    "    if end_token in raw_answer:\n",
    "            raw_answer = raw_answer.split(end_token)[0]\n",
    "\n",
    "    print(\"--- Generation Complete ---\")\n",
    "    return raw_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b94d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_question = \"How man users are using AI?\"\n",
    "generated_answer = answer_question_from_pdf(pdf_text, test_question, pipe)\n",
    "\n",
    "print(\"\\nTest Question:\")\n",
    "print_markdown(f\"**Q** {test_question}\")\n",
    "print(\"\\nGenerated Answer:\")\n",
    "print_markdown(f\"**A** {generated_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41650ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_models = {\n",
    "    \"Llama 3.2\": \"unsloth/Llama-3.2-3B-Instruct\",\n",
    "    \"Microsoft Phi-r Mini\": \"mircorosft/Phi-4-mini-instruct\",\n",
    "    \"Google Gemma 3\": \"unsloth/gemma-3-4b-it-GGUF\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e486bdd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models available to select: ['Llama 3.2', 'Microsoft Phi-r Mini', 'Google Gemma 3']\n"
     ]
    }
   ],
   "source": [
    "current_model = None\n",
    "current_pipeline = None\n",
    "print(f\"Models available to select: {list(available_models.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b351b86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llm_model(model_name):\n",
    "    global current_model_id, current_pipeline, tokenizer, model\n",
    "\n",
    "    new_model_id = available_models.get(model_name)\n",
    "    if not new_model_id:\n",
    "        return \"Invalid model selected\", None\n",
    "    \n",
    "    if new_model_id == current_model_id and current_pipeline is not None:\n",
    "        print(f\"Model {model_name} is already loaded.\")\n",
    "        return f\"{model_name} already loaded.\", current_pipeline\n",
    "    \n",
    "    print(f\"Switching to model: {model_name} ({new_model_id})...\")\n",
    "\n",
    "    # Unload preview model to free up memory and run garbage collection on GPU\n",
    "    current_pipeline = None\n",
    "    if \"model\" in locals():\n",
    "        del model\n",
    "    if \"tokenizer\" in locals():\n",
    "        del tokenizer\n",
    "    if \"pipe\" in locals():\n",
    "        del pipe    \n",
    "    torch.cuda.empty_cache()\n",
    "    import gc\n",
    "\n",
    "    gc.collect()\n",
    "    print(\"Previous model(s) unloaded (if any)\")\n",
    "\n",
    "    # Load the new model\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(new_model_id, trust_remote_code=True)\n",
    "        # Load model (4 bit quantized)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            new_model_id,\n",
    "            torch_dtype=\"auto\",\n",
    "            load_in_4bit=True,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "\n",
    "        loaded_pipeline = pipeline(\n",
    "            \"text-generation\", model=model, tokenizer=tokenizer, torch_dtype=\"auto\", device_map=\"auto\"\n",
    "        )\n",
    "\n",
    "        print(f\"Model {model_name} loaded successfully!\")\n",
    "\n",
    "        current_model_id = new_model_id\n",
    "        current_pipeline = loaded_pipeline\n",
    "        return f\"{model_name} loaded successfully!\", loaded_pipeline\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model {model_name}: {e}\")\n",
    "        current_model_id = None\n",
    "        current_pipeline = None\n",
    "        return f\"Error loading {model_name}: {e}, None\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "52362998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function relies on the global 'current_pipeline'\n",
    "def handle_submit(question):\n",
    "    if not current_pipeline:\n",
    "        return \"Error: No model is currently loaded.  Please select a model from the dropdown.\"\n",
    "    if not pdf_text:\n",
    "        return \"Error: PDF text is not loaded.\"\n",
    "    if not question:\n",
    "        return \"Please enter a question.\"\n",
    "    \n",
    "    print(f\"Handling submission for question: '{question}' using {current_model_id}\")\n",
    "    answer = answer_question_from_pdf(pdf_text, question, current_pipeline)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ceed258",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building Gradio Interface\")\n",
    "\n",
    "# Build page header text\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\n",
    "        f\"\"\"\n",
    "        # PDF Q & A Bot Using Huggingface Open-Source Models\n",
    "        Ask questions about the document ('{pdf_filename} if loaded, {len(pdf_text)} chars.)\n",
    "        Select an open-source LLM to answer your questions.\n",
    "        ***Note*** Switch models will take time as the new model will need to be dowbnloaded and loaded into the GPU\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # Build dropdown with available models to load\n",
    "    with gr.Row():\n",
    "        model_markdown = gr.Dropdown(\n",
    "            choices=list(available_models.keys()),\n",
    "            label=\"Select LLM\",\n",
    "            value=list(available_models.keys())[0] # Default to the first model\n",
    "        )\n",
    "        status_textbox = gr.Textbox(label=\"Model Status\", interactive=False)\n",
    "\n",
    "    question_textbox = gr.Textbox(\n",
    "        label=\"Your Question\", lines=2, placeholder=\"Enter your question about the document here...\"\n",
    "    )\n",
    "    submit_button = gr.Button(\"Submit Question\", variant=\"primary\")\n",
    "    answer_textbox = gr.Textbox(label=\"Answer\", lines=5, interactive=False)\n",
    "\n",
    "    # On button click, call the submit handler\n",
    "    submit_button.click(\n",
    "        fn = handle_submit,\n",
    "        inputs= [question_textbox],\n",
    "        outputs=[answer_textbox]\n",
    "    )\n",
    "\n",
    "    # Initial model load before launching Gradio for simplicity\n",
    "    initial_model_name = list(available_models.keys())[0]\n",
    "    print(f\"Performing initial load of default model: {initial_model_name}\")\n",
    "    status, _ = load_llm_model(initial_model_name)\n",
    "    status_textbox.value = status\n",
    "    print(\"Initial load complete\")\n",
    "\n",
    "# Launch Gradio app\n",
    "print(\"Launching Gradio demo\")\n",
    "demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d239d72a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b818194b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
